Spark Session:
SparkSession provides a single point of entry to interact with underlying Spark 
functionality and allows programming Spark with DataFrame and Dataset APIs.

Whereas in Spark 2.0 the same effects can be achieved through SparkSession, 
without expliciting creating SparkConf, SparkContext or SQLContext, 
as theyâ€™re encapsulated within the SparkSession. 

two data sets join in spark:




sort tuple list by second element:
list:
("Person 1",10)
("Person 2",8)
("Person 3",12)
("Person 4",20)
my_list.sort(key=lambda x: x[1])


we have some wrong code, when using Chinese character in the output file:
we can use ' '.join like this to solve the problem
res.append('\t'.join(data))


when we use map or flatmap, if we have data, we should return something.
we can't just return, or we will get NoneType error.
so we could return ' ', then we use filter to filter the ' ' value.


when we out put the result, we should out the result as a string, and use ',' to separate the fields.
